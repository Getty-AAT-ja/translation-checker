import os
import time
from pathlib import Path

import pandas as pd
from openai import OpenAI

"""
chatgpt_judge.py  
===================
This script reads *input.csv* and evaluates translation quality with an OpenAI model.
If *correct_label.csv* (no header, two columns: **URL**, **correct_label**) is present,
it overwrites the value in the ``å…ƒãƒ©ãƒ™ãƒ«`` column by matching on the URL column
(default column name ``URL``).  

Key changes from the previous version
-------------------------------------
1. Added ``CORRECT_LABEL_PATH`` and merging logic to replace inaccurate labels.
2. Added ``URL_COLUMN_NAME`` constant for flexibility if your column is named
   something other than ``URL``.
3. Reâ€‘structured the configuration section and removed the hardâ€‘coded API key.
"""

# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------
INPUT_PATH = "Getty AAT-ja-2050425äº€ç”°ã•ã‚“ä½œæ¥­ç”¨.csv"
CORRECT_LABEL_PATH = "correct_label.csv"  # No header, [URL, correct_label]
OUTPUT_PATH = "output_with_scores_4.csv"
LOG_PATH = "log.txt"

# Set this to your URL column name in both CSVs if it differs from "URL"
URL_COLUMN_NAME = "ãƒªã‚½ãƒ¼ã‚¹"

# -----------------------------------------------------------------------------
# OpenAI client initialisation  (expects OPENAI_API_KEY in environment)
# -----------------------------------------------------------------------------
client = OpenAI(api_key = os.environ["OPENAI_API_KEY"])

# -----------------------------------------------------------------------------
# Load input data and, if available, overwrite å…ƒãƒ©ãƒ™ãƒ« using correct_label.csv
# -----------------------------------------------------------------------------

def load_and_correct_labels() -> pd.DataFrame:
    """Read *input.csv* and, if present, merge in corrected labels."""
    # Load main data
    df = pd.read_csv(INPUT_PATH)

    # Input validation
    if URL_COLUMN_NAME not in df.columns:
        raise KeyError(
            f"'{URL_COLUMN_NAME}' column is required in {INPUT_PATH} to match URLs"
        )

    # Try to load corrected labels
    if Path(CORRECT_LABEL_PATH).exists():
        df_correct = pd.read_csv(
            CORRECT_LABEL_PATH, header=None, names=[URL_COLUMN_NAME, "correct_label"]
        )
        # Merge on URL and overwrite ãƒ©ãƒ™ãƒ«åŸæ–‡ where a correction exists
        df = df.merge(df_correct, on=URL_COLUMN_NAME, how="left")
        if "ãƒ©ãƒ™ãƒ«åŸæ–‡" not in df.columns:
            # If ãƒ©ãƒ™ãƒ«åŸæ–‡ column didnâ€™t exist, create it
            df["ãƒ©ãƒ™ãƒ«åŸæ–‡"] = df["correct_label"]
        else:
            df.loc[~df["correct_label"].isna(), "ãƒ©ãƒ™ãƒ«åŸæ–‡"] = df.loc[
                ~df["correct_label"].isna(), "correct_label"
            ]
        df.drop(columns=["correct_label"], inplace=True)
    else:
        print(
            f"Warning: {CORRECT_LABEL_PATH} not found. Proceeding without label correction."
        )

    # âœ… æ–‡çŒ®1ã€œ3ã™ã¹ã¦ã«éç©ºã®å€¤ãŒã‚ã‚‹è¡Œã ã‘ã‚’æ®‹ã™ï¼ˆNaN + ç©ºæ–‡å­—å¯¾å¿œï¼‰
    required_columns = ["æ–‡çŒ®1", "æ–‡çŒ®2", "æ–‡çŒ®3"]
    for col in required_columns:
        df = df[df[col].notna() & (df[col].astype(str).str.strip() != "")]
    return df


def build_prompt(row: pd.Series) -> str:
    """Compose the evaluation prompt for a single row."""
    return f"""
ä»¥ä¸‹ã®å°‚é–€ç”¨èªè¾æ›¸ã®ç¿»è¨³æƒ…å ±ã«ã¤ã„ã¦ã€8ã¤ã®è¦³ç‚¹ã‹ã‚‰5ç‚¹æº€ç‚¹ã§è©•ä¾¡ã—ã€ãã‚Œãã‚Œç†ç”±ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚

ã€å…ƒãƒ©ãƒ™ãƒ«ï¼ˆè‹±èªï¼‰ã€‘:
{row['ãƒ©ãƒ™ãƒ«åŸæ–‡']}

ã€æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ï¼ˆäººè¨³ï¼‰ã€‘:
{row['æ—¥æœ¬èªãƒ©ãƒ™ãƒ«(äººè¨³)']}

ã€è§£èª¬æ–‡ï¼ˆè‹±èªï¼‰ã€‘:
{row['è§£èª¬åŸæ–‡']}

ã€è§£èª¬æ–‡ï¼ˆäººè¨³ãƒ»ä¿®æ­£ï¼‰ã€‘:
{row['è§£èª¬æ–‡(äººè¨³)']}

è©•ä¾¡é …ç›®:
(1) æƒ…å ±ã®æŠœã‘ãŒç„¡ã„ã‹ã€‚ã¤ã¾ã‚Šã€ã€Œè§£èª¬æ–‡ï¼ˆè‹±èªï¼‰ã€ã«ã‚ã‚‹æƒ…å ±ãŒãã®è¨³ã€Œè§£èª¬æ–‡ï¼ˆäººè¨³ãƒ»ä¿®æ­£ï¼‰ã€ã§æŠœã‘è½ã¡ã¦ãªã„ã‹ã€‚ãªã‘ã‚Œã°5ç‚¹æº€ç‚¹ã€‚
(2) è‡ªç„¶ãªæ—¥æœ¬èªæ–‡ã§ã‚ã‚‹ã‹ã€‚ã€Œè§£èª¬æ–‡ï¼ˆäººè¨³ãƒ»ä¿®æ­£ï¼‰ã€ã«å¯¾ã—ã¦è©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
(3) å¯¾è±¡ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ååˆ†ãªèª¬æ˜ã‹ã€‚ã€Œè§£èª¬æ–‡ï¼ˆè‹±èªï¼‰ã€ã«å¯¾ã—ã¦è©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
(4) é–¢é€£æ·±ã„å°‚é–€çŸ¥è­˜ã‚’é©åˆ‡ã«ç”¨ã„ã¦èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã‹ï¼ˆç†ç”±ã«é–¢é€£ã™ã‚‹å°‚é–€ã‚’åˆ—æŒ™ã—ã¦ãã ã•ã„ï¼‰ã€‚ã€Œè§£èª¬æ–‡ï¼ˆè‹±èªï¼‰ã€ã«å¯¾ã—ã¦è©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
(5) äº‹å®Ÿã§ã¯ãªã„å†…å®¹ã‚’å«ã‚“ã§ã„ãªã„ã‹ã€‚å°‚é–€çŸ¥è­˜ã«åŸºã¥ã„ã¦åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚ã€Œè§£èª¬æ–‡ï¼ˆäººè¨³ãƒ»ä¿®æ­£ï¼‰ã€ã«å¯¾ã—ã¦è©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
(6) å…ƒãƒ©ãƒ™ãƒ«ã¨æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ãŒåŒã˜å¯¾è±¡ã‚’æŒ‡ã—ã¦ã„ã‚‹ã‹ã€‚ã‚ºãƒ¬ã€ã¤ã¾ã‚Šã©ã¡ã‚‰ã‹ã ã‘ãŒå«ã‚€å¯¾è±¡ãŒã‚ã£ãŸã‚Šã€æŒ‡ç¤ºå¯¾è±¡ã®æ›–æ˜§æ€§ã¯ãªã„ã‹ã€‚
(7) ã‚ã‚‹åˆ†é‡ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ç”¨èªãƒ»è¨³èªãŒã‚ã‚‹å ´åˆã€ãã‚Œã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã€‚ç‹¬è‡ªã«é€ èªã—ã¦ã„ãªã„ã‹ã€‚ã€Œæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ï¼ˆäººè¨³ï¼‰ã€ã«å¯¾ã—ã¦è©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
(8) ã‚ã‚‹åˆ†é‡ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ç”¨èªãƒ»è¨³èªãŒã‚ã‚‹å ´åˆã€ãã‚Œã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã€‚ç‹¬è‡ªã«é€ èªã—ã¦ã„ãªã„ã‹ã€‚ã€Œè§£èª¬æ–‡ï¼ˆäººè¨³ãƒ»ä¿®æ­£ï¼‰ã€ã«å¯¾ã—ã¦è©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

å½¢å¼ã¯ä»¥ä¸‹ã«å¾“ã£ã¦ãã ã•ã„ï¼š

(1) ã‚¹ã‚³ã‚¢: X/5 ç†ç”±: ...
(2) ã‚¹ã‚³ã‚¢: X/5 ç†ç”±: ...
...
(7) ã‚¹ã‚³ã‚¢: X/5 ç†ç”±: ...
"""


def parse_response(text: str) -> dict:
    """Extract scores/reasons from the model response as a dict."""
    import re

    results = {}
    for i in range(1, 9):
        score_match = re.search(rf"\({i}\) ã‚¹ã‚³ã‚¢:\s*([0-5])/5", text)
        reason_match = re.search(
            rf"\({i}\) ã‚¹ã‚³ã‚¢:\s*[0-5]/5\s*ç†ç”±:\s*(.+?)(?=\(\d+\) ã‚¹ã‚³ã‚¢:|$)",
            text,
            re.DOTALL,
        )
        results[f"Score_{i}"] = int(score_match.group(1)) if score_match else None
        results[f"Reason_{i}"] = (
            reason_match.group(1).strip() if reason_match else ""
        )
    return results


# -----------------------------------------------------------------------------
# Main processing
# -----------------------------------------------------------------------------

def main():
    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿è¾¼ã¿ï¼‰
    df = load_and_correct_labels()
    print("ğŸŸ¦ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰ã®èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"è¡Œæ•°: {len(df)}")
    print("åˆ—å:", df.columns.tolist())
    print("å…ˆé ­5è¡Œ:\n", df.head())

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å¾©å…ƒå‡¦ç†ã¸
    if Path(OUTPUT_PATH).exists():
        df_prev = pd.read_csv(OUTPUT_PATH)
        print(f"ğŸŸ© å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« {OUTPUT_PATH} ã‚’èª­ã¿è¾¼ã¿ï¼ˆ{len(df_prev)} è¡Œï¼‰")
        print("å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­5è¡Œ:\n", df_prev.head())

        # çªãåˆã‚ã›å‰ã« key åˆ—ï¼ˆURL or ãƒªã‚½ãƒ¼ã‚¹ï¼‰ç¢ºèª
        print("df[URL_COLUMN_NAME] ã‚µãƒ³ãƒ—ãƒ«:", df[URL_COLUMN_NAME].head())
        print("df_prev[URL_COLUMN_NAME] ã‚µãƒ³ãƒ—ãƒ«:", df_prev[URL_COLUMN_NAME].head())

        # set_indexã—ã¦ä¸Šæ›¸ã
        df_prev.set_index(URL_COLUMN_NAME, inplace=True)
        df.set_index(URL_COLUMN_NAME, inplace=True)

        for i in range(1, 9):
            score_col = f"Score_{i}"
            reason_col = f"Reason_{i}"
            if score_col in df_prev.columns:
                df[score_col] = df_prev[score_col]
            if reason_col in df_prev.columns:
                df[reason_col] = df_prev[reason_col]

        df.reset_index(inplace=True)
    else:
        print("ğŸŸ¥ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚æ–°è¦å‡¦ç†ã‚’è¡Œã„ã¾ã™")
        for i in range(1, 9):
            df[f"Score_{i}"] = None
            df[f"Reason_{i}"] = ""

    # ã‚¹ã‚³ã‚¢åˆ—ç¢ºèª
    print("Score_1 ã®æ¬ æè¡Œæ•°:", df["Score_1"].isna().sum())

    incomplete_mask = df["Score_1"].isna()
    if not incomplete_mask.any():
        print("âœ… ã™ã¹ã¦ã®è¡ŒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚")
        return

    START_INDEX = incomplete_mask.idxmax()
    iloc_start = df.index.get_loc(START_INDEX)
    print(f"â–¶ å‡¦ç†ã‚’ {iloc_start} è¡Œç›®ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰ã‹ã‚‰å†é–‹ã—ã¾ã™ã€‚")

    # ChatGPT evaluation loop
    with open(LOG_PATH, "a", encoding="utf-8") as log:  # â† è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
        for idx, row in df.iloc[iloc_start:].iterrows():
            print(f"[{idx}] å‡¦ç†ä¸­ãƒ©ãƒ™ãƒ«: {row['ãƒ©ãƒ™ãƒ«åŸæ–‡']}")
            prompt = build_prompt(row)
            try:
                response = client.chat.completions.create(
                    model="gpt-4o-mini",  # adjust model as needed
                    messages=[
                        {
                            "role": "system",
                            "content": "ã‚ãªãŸã¯è‹±èªã¨æ—¥æœ¬èªãŒå ªèƒ½ãªç¿»è¨³ã®ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã§ã€èŠ¸è¡“ã‚’ä¸­å¿ƒã¨ã—ã¦æ§˜ã€…ãªå°‚é–€çŸ¥è­˜ã‚’æŒã£ã¦ã„ã¾ã™ã€‚",
                        },
                        {"role": "user", "content": prompt},
                    ],
                    temperature=0.2,
                    max_tokens=2000,
                )
                output = response.choices[0].message.content
                log.write(f"===== Index: {idx} =====\n{output}\n\n")

                parsed = parse_response(output)
                for key, value in parsed.items():
                    df.at[idx, key] = value

                # âœ… å„è¡Œã”ã¨ã«ä¿å­˜ï¼ˆä¸­æ–­å¯¾ç­–ï¼‰
                df.to_csv(OUTPUT_PATH, index=False)

                time.sleep(0.5)  # polite rate limit
            except Exception as e:
                print(f"Error at index {idx}: {e}")
                continue

if __name__ == "__main__":
    main()